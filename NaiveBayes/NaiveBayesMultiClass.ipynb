{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a227ad27fbee583d91020997b5ead496",
     "grade": false,
     "grade_id": "cell-markdown-task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2 â€“ Naive Bayes Multilabel Classification\n",
    "(10 points)\n",
    "\n",
    "Implement a Naive Bayes classifier which is able to assign multiple classes to a single document by finalizing the two given classes. The `MultiLabelLearner` class acts like a builder for the `MultiLabelClassifier` instances. That means that the learner gets the number of classes during its creation and the `learnExample` method of the learner is called once for each document of the training set. Internally, the learner should gather all statistics that are necessary for the classifier when processing the training examples.\n",
    "After the learner saw all training documents, the `createClassifier` method is called which creates an instance of the `MultiLabelClassifier` class and initializes it with the statistics gathered before. \n",
    "The classification itself is carried out by the `classify` method which takes an unknown document and assigns it a set of classes learned before.\n",
    "\n",
    "#### Hints\n",
    "\n",
    "- Please do not forget to preprocess your documents. What exactly the preprocessing does is up to you.\n",
    "- The classification should be based on the naive Bayes classification. You may want to reuse code from exercise 1.\n",
    "- In our datasets, each document has *at least one* class. You may want to take this information into account.\n",
    "- The evaluation will use micro precision, micro recall and micro F1-measure (also named F1-score).\n",
    "- The evaluation in the hidden tests has three stages. \n",
    "  1. Your solution will get 4 points as soon as it is better than the baselines. The baselines are:\n",
    "     - For each class, a classifier that always returns this class.\n",
    "     - A random guesser that returns a random class.\n",
    "  2. If your solution has an F1-score >= 0.7, you will get 3 more points.\n",
    "  3. If your solution has an F1-score >= 0.8, you will get 3 more points.\n",
    "- You can download the [multi-class-train.tsv](https://hobbitdata.informatik.uni-leipzig.de/teaching/SNLP/classification/multi-class-train.tsv) file. It comprises one document per line. The first part comprises the classes (separated with a `\", \"` string), followed by a tab character (`\\t`). The remaining content of the line is the text of the document.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "- You are free to use a different IDE to develop your solution. However, you have to copy the solution into this notebook to submit it.\n",
    "- Do not add additional external libraries.\n",
    "- Interface\n",
    "  - You can use _[TAB]_ for autocompletion and _[SHIFT]_+_[TAB]_ for code inspection.\n",
    "  - Use _Menu_ -> _View_ -> _Toggle Line Numbers_ for debugging.\n",
    "  - Check _Menu_ -> _Help_ -> _Keyboard Shortcuts_.\n",
    "- Known issues\n",
    "  - All global variables will be set to void after an import.\n",
    "  - Missing spaces arround `%` (Modulo) can cause unexpected errors so please make sure that you have added spaces around every `%` character.\n",
    "- Finish\n",
    "  - Save your solution by clicking on the _disk icon_.\n",
    "  - Make sure that all necessary imports are listed at the beginning of your cell.\n",
    "  - Run a final check of your solution by\n",
    "    - click on _restart the kernel, then re-run the whole notebook_ (the fast forward arrow in the tool bar)\n",
    "    - wait fo the kernel to restart and execute all cells (all executable cells should have numbers in front of them instead of a `[*]`) \n",
    "    - Check all executed cells for errors. If an exception is thrown, please check your code. Note that although the error might look cryptic, until now we never encounter that an exception was caused without a valid reason inside of the submitted code. A good way to check the code is to copy the solution into a new class in your favorite IDE and check\n",
    "      - errors reported by the IDE\n",
    "      - imports the IDE adds to your code which might be missing in your submission.\n",
    "  - Finally, choose _Menu_ -> _File_ -> _Close and Halt_.\n",
    "  - Do not forget to _Submit_ your solution in the _Assignments_ view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55e995d0be52a1c40c779d8e4c3e0ffb",
     "grade": false,
     "grade_id": "cell-code",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled\n"
     ]
    }
   ],
   "source": [
    "// package NaiveBayesian;\n",
    "\n",
    "import java.util.ArrayList;\n",
    "import java.util.Arrays;\n",
    "import java.util.Collections;\n",
    "import java.util.HashMap;\n",
    "import java.util.HashSet;\n",
    "import java.util.List;\n",
    "import java.util.Map;\n",
    "import java.util.Set;\n",
    "\n",
    "//YOUR CODE HERE\n",
    "\n",
    "/**\n",
    " * Classifier implementing naive Bayes classification for a multilabel\n",
    " * classification.\n",
    " */\n",
    "public class MultiLabelClassifier {\n",
    "\t// YOUR CODE HERE\n",
    "\tList<String> classNameList;\n",
    "\tint totalDocCount;\n",
    "\tMap<String, int[]> classFreqMap;\n",
    "\tList<String> vocab1;\n",
    "\tint[] docCountEachClass;\n",
    "\tdouble[] priorProbs;\n",
    "\tMap<String, Integer> classDocsCount;\n",
    "\n",
    "\tMultiLabelClassifier() {\n",
    "\t};\n",
    "\n",
    "\tpublic MultiLabelClassifier(List<String> classNameList, int totalDocCount, Map<String, int[]> classFreqMap,\n",
    "\t\t\tList<String> vocab1, int[] docCountEachClass, Map<String, Integer> classDocsCount) {\n",
    "\t\t// TODO Auto-generated constructor stub\n",
    "\t\tthis.classFreqMap = classFreqMap;\n",
    "\t\tthis.classNameList = classNameList;\n",
    "\t\tthis.totalDocCount = totalDocCount;\n",
    "\t\tthis.vocab1 = vocab1;\n",
    "\t\tthis.docCountEachClass = docCountEachClass;\n",
    "\t\tthis.classDocsCount = classDocsCount;\n",
    "\t\t\n",
    "\t\tpriorProbs = new double[docCountEachClass.length];\n",
    "\t\t// Now we have the prior probs of each class\n",
    "\t\tfor (int i = 0; i < priorProbs.length; i++) {\n",
    "\t\t\tpriorProbs[i] = (double) docCountEachClass[i] / totalDocCount;\n",
    "\t\t}\n",
    "\n",
    "\t}\n",
    "\n",
    "\tString[] stopwords = { \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "\t\t\t\"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
    "\t\t\t\"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
    "\t\t\t\"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "\t\t\t\"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "\t\t\t\"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "\t\t\t\"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "\t\t\t\"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "\t\t\t\"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "\t\t\t\"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\" };\n",
    "\n",
    "\tList<String> stopWordsList = Arrays.asList(stopwords);\n",
    "\n",
    "\tpublic Set<String> preprocessing(String text) {\n",
    "//\t\tSystem.out.println(text);\n",
    "\t\tdouble[][] condProbArray = new double[vocab1.size()][classNameList.size()];\n",
    "\n",
    "\t\tString[] tokens = text.toLowerCase().replaceAll(\"[^ a-zA-Z0-9]\", \"\").split(\" \");\n",
    "\t\tdouble score[] = new double[classNameList.size()];\n",
    "\n",
    "\t\tfor (String token : tokens) {\n",
    "\t\t\t// check if the token is not a stop word\n",
    "\t\t\tif (!stopWordsList.contains(token)) {\n",
    "\t\t\t\tint indexToken = 0, freqToken = 0;\n",
    "\t\t\t\tboolean flag = false;\n",
    "\n",
    "\t\t\t\tif (vocab1.contains(token)) {\n",
    "\t\t\t\t\tindexToken = vocab1.indexOf(token);\n",
    "\t\t\t\t\tflag = true; // that token is in vocab\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\t\t\tdouble condProb = 0.0;\n",
    "\n",
    "\t\t\t\tint i = 0;\n",
    "\t\t\t\t// calculate the score of each class for this token.\n",
    "\t\t\t\tfor (String c : classNameList) {\n",
    "\t\t\t\t\t// this will hold the total length of each class\n",
    "\t\t\t\t\tint classSize = classDocsCount.get(c);\n",
    "\n",
    "\t\t\t\t\t// pull out the array of frequence for each term for that\n",
    "\t\t\t\t\t// class\n",
    "\t\t\t\t\tint[] wordList = classFreqMap.get(c);\n",
    "\n",
    "\t\t\t\t\tdouble logToken;\n",
    "\n",
    "\t\t\t\t\tif (flag) {\n",
    "\n",
    "\t\t\t\t\t\tcondProb = condProbArray[indexToken][classNameList.indexOf(c)];\n",
    "\t\t\t\t\t\tif (condProb > 0) {\n",
    "\t\t\t\t\t\t\t// as the condprob is already there, calculate its\n",
    "\t\t\t\t\t\t\t// log\n",
    "\t\t\t\t\t\t\tlogToken = Math.log(condProb);\n",
    "\t\t\t\t\t\t\tscore[i] += logToken;\n",
    "\t\t\t\t\t\t\ti++;\n",
    "\t\t\t\t\t\t\tcontinue;\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t// if the condProb is not there, calculate\n",
    "\t\t\t\t\t\telse {\n",
    "\t\t\t\t\t\t\t// our aim here is to calculate only the freqToken\n",
    "\t\t\t\t\t\t\t// so it can be zero if the token is not in vocab.\n",
    "\t\t\t\t\t\t\tfreqToken = wordList[indexToken];\n",
    "\t\t\t\t\t\t\tcondProbArray[indexToken][classNameList.indexOf(c)] = (double) (freqToken + 1)\n",
    "\t\t\t\t\t\t\t\t\t/ (classSize + vocab1.size());\n",
    "\t\t\t\t\t\t\tlogToken = Math.log(condProbArray[indexToken][classNameList.indexOf(c)]);\n",
    "\t\t\t\t\t\t\tscore[i] += logToken;\n",
    "\t\t\t\t\t\t\ti++;\n",
    "\t\t\t\t\t\t\tcontinue;\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t\t// here if token is not in vocab then freq would be zero.\n",
    "\n",
    "\t\t\t\t\t// use this freq for condProbArray\n",
    "\t\t\t\t\telse {\n",
    "\n",
    "\t\t\t\t\t\tcondProb = (double) (freqToken + 1) / (classSize + vocab1.size());\n",
    "\t\t\t\t\t\tlogToken = Math.log(condProb);\n",
    "\t\t\t\t\t\tscore[i] += logToken;\n",
    "\t\t\t\t\t\ti++;\n",
    "\t\t\t\t\t\tcontinue;\n",
    "\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\t\t}\n",
    "\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tfor (int i = 0; i < score.length; i++) {\n",
    "\t\t\tscore[i] += Math.log(priorProbs[i]);\n",
    "//\t\t\tSystem.out.println(\"or score: \"+score[i]);\n",
    "\t\t}\n",
    "\t\t\n",
    "//\t\tfor (int i = 0; i < score.length; i++) {\n",
    "//\t\t\tSystem.out.println(score[i]+\" \"+classNameList.get(i));\n",
    "//\t\t}\n",
    "\t\t\n",
    "//\t\tArrays.sort(score);\n",
    "\t\tList<Double> scoreList = new ArrayList<>();\n",
    "\t\t\n",
    "\t\tfor(double scoreElement: score){\n",
    "\t\t\tscoreList.add(scoreElement);\n",
    "\t\t}\n",
    "\t\tCollections.sort(scoreList);\n",
    "\t\tdouble maxScore = scoreList.get(scoreList.size()-1);\n",
    "\t\tSet<String> results = new HashSet();\n",
    "\t\tfor(int k=0;k<score.length;k++){\n",
    "//\t\t\tSystem.out.println(score[k]);\n",
    "\t\t\tif(maxScore-score[k]<1.5){\n",
    "\t\t\t\tresults.add(classNameList.get(k));\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t\n",
    "//\t\tSystem.out.println(\"the final result: \"+results);\n",
    "\t\treturn results;\n",
    "\t}\n",
    "\n",
    "\t/**\n",
    "\t * Classifies the given document and returns the class names.\n",
    "\t */\n",
    "\tpublic Set<String> classify(String text) {\n",
    "\t\tSet<String> score = preprocessing(text);\n",
    "\t\t\n",
    "\t\tfor(int i=0;i<classNameList.size();i++){\n",
    "//\t\t\tSystem.out.println(classNameList.get(i));\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t/*for(int i =0;i<score.length;i++){\n",
    "\t\t\tSystem.out.println(text+\" score of \"+ i+ score[i]);\n",
    "\t\t}\n",
    "\t\tSet<String> results = new HashSet<>();\n",
    "\t\tdouble maxScore = score[(score.length)-1];\n",
    "\t\tresults.add(classNameList.get(score.length-1));\n",
    "\t\tSystem.out.println(results+\"\"+maxScore);\n",
    "\t\tfor(int d=0;d<score.length-1;d++){\n",
    "\t\t\tif(maxScore-score[d]<0.8){\n",
    "\t\t\t\tresults.add(classNameList.get(d));\n",
    "\t\t\t\tSystem.out.println(results);\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\t*/\n",
    "//\t\tSystem.out.println(text);\n",
    "\t\t// YOUR CODE HERE\n",
    "//\t\tresults.add(\"history\");\n",
    "\t\treturn score;\n",
    "\t}\n",
    "}\n",
    "\n",
    "/**\n",
    " * Learner (or Builder) class for a naive Bayes multilabel classifier.\n",
    " */\n",
    "class MultiLabelLearner {\n",
    "\t// YOUR CODE HERE\n",
    "\tList<String> classNameList = new ArrayList<>();\n",
    "\tint totalDocCount = 0; // stores the total number of classes.\n",
    "\tint[] docCountEachClass; // stores the count of each class at index.\n",
    "\tList<String> vocab1 = new ArrayList<>();\n",
    "\n",
    "\t// this map stores the class and all the docs related to the class in one\n",
    "\t// string object\n",
    "\tMap<String, String> mapClassText = new HashMap<>();\n",
    "\n",
    "\t/**\n",
    "\t * Constructor taking the number of classes the classifier should be able to\n",
    "\t * distinguish.\n",
    "\t */\n",
    "\tpublic MultiLabelLearner(Set<String> classes) {\n",
    "\t\t// YOUR CODE HERE\n",
    "\n",
    "\t\tfor (String clazz : classes) {\n",
    "\t\t\tclassNameList.add(clazz);\n",
    "\t\t}\n",
    "\t\t\n",
    "\n",
    "\t\tdocCountEachClass = new int[classNameList.size()];\n",
    "\t}\n",
    "\n",
    "\tString[] stopwords = { \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "\t\t\t\"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
    "\t\t\t\"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
    "\t\t\t\"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "\t\t\t\"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "\t\t\t\"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "\t\t\t\"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "\t\t\t\"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "\t\t\t\"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "\t\t\t\"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\" };\n",
    "\n",
    "\tList<String> stopWordsList = Arrays.asList(stopwords);\n",
    "\n",
    "\t// This array belongs to each class containing for each class, its term\n",
    "\t// count array.\n",
    "\tMap<String, int[]> classFreqMap = new HashMap<>();\n",
    "\t// This array belongs to mapping of count of term with vocab\n",
    "\tint[] tokenCountEachVocab;\n",
    "\n",
    "\t/**\n",
    "\t * The method used to learn the training examples. It takes the names of the\n",
    "\t * classes as well as the text of the training document.\n",
    "\t */\n",
    "\tpublic void learnExample(Set<String> classes, String text) {\n",
    "\t\t// YOUR CODE HERE\n",
    "\t\ttotalDocCount++;\n",
    "\n",
    "\t\t// here we have calculated the array with no. of documents belonging to\n",
    "\t\t// a class.\n",
    "//\t\tif(classes.size()==1){\n",
    "\t\tfor (String clazz : classes) {\n",
    "\t\t\tint indexClass = classNameList.indexOf(clazz);\n",
    "\t\t\tdocCountEachClass[indexClass] += 1;\n",
    "\n",
    "\t\t\t// Performance measure test with the preprocessed textual content\n",
    "\t\t\tString text_processed = text.toLowerCase().replaceAll(\"[^ a-zA-Z0-9]\", \"\");\n",
    "\n",
    "\t\t\t// we preprocess a text for that class. This will concatenate the\n",
    "\t\t\t// texts for one class and also prepare the vocab.\n",
    "\t\t\taddVocab(text_processed);\n",
    "\n",
    "\t\t\tpreprocessing(clazz, text_processed);\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tvoid addVocab(String text) {\n",
    "\t\tString tokens[] = text.split(\" \");\n",
    "\t\tfor (String token : tokens) {\n",
    "\t\t\tif (!stopWordsList.contains(token)) {\n",
    "\t\t\t\tif (!vocab1.contains(token)) {\n",
    "\t\t\t\t\tvocab1.add(token);\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tvoid preprocessing(String clazz, String text) {\n",
    "\t\t// This method, will assign a concatenated string to class.\n",
    "\t\tString tempVect = new String();\n",
    "\t\tif (mapClassText.containsKey(clazz)) {\n",
    "\t\t\ttempVect = mapClassText.get(clazz);\n",
    "\t\t}\n",
    "\n",
    "\t\tmapClassText.put(clazz, tempVect.concat(\" \"+text));\n",
    "\n",
    "\t}\n",
    "\n",
    "\tMap<String, Integer> classDocsCount = new HashMap<>();\n",
    "\n",
    "\tint[] prepareArray(String[] tokens) {\n",
    "\t\ttokenCountEachVocab = new int[vocab1.size()];\n",
    "\n",
    "\t\tfor (String token : tokens) {\n",
    "\t\t\tif (!stopWordsList.contains(token) && !token.equals(\"\")) {\n",
    "\t\t\t\tint indexTokenVocab = vocab1.indexOf(token);\n",
    "\t\t\t\ttokenCountEachVocab[indexTokenVocab] += 1;\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\t\treturn tokenCountEachVocab;\n",
    "\t}\n",
    "\n",
    "\t/**\n",
    "\t * Creates a MultiLabelClassifier instance based on the statistics gathered\n",
    "\t * from the training example.\n",
    "\t */\n",
    "\tpublic MultiLabelClassifier createClassifier() {\n",
    "\t\t// YOUR CODE HERE\n",
    "\t\tfor (String clazz : classNameList) {\n",
    "\t\t\tString textClass = mapClassText.get(clazz);\n",
    "\t\t\tString[] tokens = textClass.trim().split(\" \");\n",
    "\t\t\tint[] tokenSpace = prepareArray(tokens);\n",
    "\t\t\tint sum=0;\n",
    "\t\t\tfor(int t:tokenSpace){\n",
    "\t\t\t\tsum+=t;\n",
    "\t\t\t}\n",
    "\t\t\t// This will store for each class, no. of tokens belonging to it.\n",
    "\t\t\tclassDocsCount.put(clazz, sum);\n",
    "\t\t\tclassFreqMap.put(clazz, tokenSpace);\n",
    "\t\t}\n",
    "\n",
    "\t\tMultiLabelClassifier classifier = new MultiLabelClassifier(classNameList, totalDocCount, classFreqMap, vocab1,\n",
    "\t\t\t\tdocCountEachClass, classDocsCount);\n",
    "\n",
    "\t\treturn classifier;\n",
    "\t}\n",
    "}\n",
    "// This line should make sure that compile errors are directly identified when\n",
    "// executing this cell\n",
    "// (the line itself does not produce any meaningful result)\n",
    "new MultiLabelLearner(new HashSet<>(Arrays.asList(\"good\",\"bad\")));\n",
    "System.out.println(\"compiled\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e9c819c081918a6fde3b4ba21898440",
     "grade": false,
     "grade_id": "cell-markdown-evaluation",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "- Run the following cell to test your implementation.\n",
    "- You can ignore the cells afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29423fc4eaa30faa4c0afac6b4bbaa48",
     "grade": true,
     "grade_id": "cell-autograde-visible",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Simple example corpus ----------\n",
      "Training corpus size: 5\n",
      "Eval. corpus size   : 3\n",
      "Training took       : 1ms\n",
      "Classification took : 0ms\n",
      "classifiers           precision    recall  f1-score\n",
      "Always game         :   0.66667   0.50000   0.57143\n",
      "Always chess        :   0.33333   0.25000   0.28571\n",
      "Always history      :   0.33333   0.25000   0.28571\n",
      "Your solution       :   1.00000   0.75000   0.85714 (3 tp, 5 tn, 0 fp, 1 fn)\n",
      "  Wrong classifications are:\n",
      "    id=1 expected=[game, chess] result=[game]\n",
      "Test successfully completed.\n",
      "\n",
      "---------- Larger example corpus ----------\n",
      "Training corpus size: 600\n",
      "Eval. corpus size   : 195\n",
      "Training took       : 2975ms\n",
      "Classification took : 625ms\n",
      "classifiers           precision    recall  f1-score\n",
      "Always money-fx     :   0.43590   0.29720   0.35343\n",
      "Always nat-gas      :   0.03077   0.02098   0.02495\n",
      "Always interest     :   0.10256   0.06993   0.08316\n",
      "Always corn         :   0.06667   0.04545   0.05405\n",
      "Always ship         :   0.16923   0.11538   0.13721\n",
      "Always wheat        :   0.08718   0.05944   0.07069\n",
      "Always dlr          :   0.12821   0.08741   0.10395\n",
      "Always grain        :   0.15385   0.10490   0.12474\n",
      "Always crude        :   0.29231   0.19930   0.23701\n",
      "Your solution       :   0.94313   0.69580   0.80080 (199 tp, 1457 tn, 12 fp, 87 fn)\n",
      "  Wrong classifications are:\n",
      "    id=1 expected=[money-fx, dlr] result=[money-fx]\n",
      "    id=2 expected=[ship, crude] result=[ship]\n",
      "    id=3 expected=[money-fx, interest] result=[money-fx]\n",
      "    id=5 expected=[corn, grain] result=[grain]\n",
      "    id=7 expected=[money-fx, interest] result=[interest]\n",
      "    id=8 expected=[money-fx, dlr] result=[dlr]\n",
      "    id=11 expected=[wheat, grain] result=[grain]\n",
      "    id=13 expected=[nat-gas, crude] result=[crude]\n",
      "    id=14 expected=[wheat, grain] result=[grain]\n",
      "    id=16 expected=[wheat, grain] result=[grain]\n",
      "    id=19 expected=[money-fx, dlr] result=[money-fx]\n",
      "    id=22 expected=[money-fx, interest] result=[money-fx]\n",
      "    id=24 expected=[wheat, grain] result=[grain]\n",
      "    id=25 expected=[money-fx, interest] result=[money-fx]\n",
      "    id=26 expected=[money-fx, dlr] result=[money-fx]\n",
      "    id=28 expected=[money-fx, interest] result=[money-fx]\n",
      "    id=29 expected=[ship] result=[ship, crude]\n",
      "    id=29 expected=[ship] result=[crude]\n",
      "    id=31 expected=[ship, crude] result=[ship]\n",
      "    id=33 expected=[money-fx] result=[money-fx, interest]\n",
      "    ...\n",
      "Test successfully completed.\n"
     ]
    }
   ],
   "source": [
    "%maven org.junit.jupiter:junit-jupiter-api:5.3.1\n",
    "%maven commons-io:commons-io:2.6\n",
    "import org.junit.jupiter.api.Assertions;\n",
    "import org.opentest4j.AssertionFailedError;\n",
    "import java.util.stream.Collectors;\n",
    "import java.util.Map.Entry;\n",
    "import org.apache.commons.io.FileUtils;\n",
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "\n",
    "/**\n",
    " * Simple structure to store the classes and the text of a document.\n",
    " */\n",
    "public class ClassifiedDocument {\n",
    "    public final Set<String> classes;\n",
    "    public final String text;\n",
    "    public ClassifiedDocument(Set<String> classes, String text) {\n",
    "        this.classes = classes;\n",
    "        this.text = text;\n",
    "    }\n",
    "}\n",
    "/**\n",
    " * Simple method for reading classification examples from a file as a list of (classes, text) pairs.\n",
    " */\n",
    "public static List<ClassifiedDocument> readClassData(String filename) throws IOException {\n",
    "    return FileUtils.readLines(new File(filename), \"utf-8\").stream().map(s -> s.split(\"\\t\"))\n",
    "            .filter(s -> s.length > 1)\n",
    "            .map(s -> new ClassifiedDocument(new HashSet<>(Arrays.asList(s[0].split(\", \"))), s[1]))\n",
    "            .collect(Collectors.toList());\n",
    "}\n",
    "\n",
    "/**\n",
    " * Method for cecking the given classifier. The method expects training and evaluation data.\n",
    " * The data should have a String array for each document in which the first cell of the\n",
    " * array contains the class while the second cell contains the text of the document. During\n",
    " * the check, some statistics like the F1-scores of different baseline classifiers are\n",
    " * printed. Finally, the calculated F1-score is returned.\n",
    " *\n",
    " * @param trainingCorpus the data that is used for training the classifier. \n",
    " * @param evaluationCorpus the data that is used for evaluating the classifier. \n",
    " * @param minAccuracy minimum accuracy the classifier should achieve.\n",
    " * @return the F1-score achieved by the classifier\n",
    " */\n",
    "public static double checkClassifier(List<ClassifiedDocument> trainingCorpus,\n",
    "        List<ClassifiedDocument> evaluationCorpus, double minF1Score) {\n",
    "    double f1 = 0;\n",
    "    try {\n",
    "        System.out.print(\"Training corpus size: \");\n",
    "        System.out.println(trainingCorpus.size());\n",
    "        System.out.print(\"Eval. corpus size   : \");\n",
    "        System.out.println(evaluationCorpus.size());\n",
    "        // Determine the classes\n",
    "        Set<String> classes = Arrays.asList(trainingCorpus, evaluationCorpus).stream().flatMap(l -> l.stream())\n",
    "                .map(d -> d.classes).flatMap(c -> c.stream()).distinct().collect(Collectors.toSet());\n",
    "        // Determine the number of instances per class in the evaluation set\n",
    "        Map<String, Long> evalClassCounts = evaluationCorpus.stream().map(d -> d.classes).flatMap(c -> c.stream())\n",
    "                .collect(Collectors.groupingBy(c -> c, Collectors.counting()));\n",
    "        for (String clazz : classes) {\n",
    "            if (!evalClassCounts.containsKey(clazz)) {\n",
    "                evalClassCounts.put(clazz, 0L);\n",
    "            }\n",
    "        }\n",
    "        long expectedClassSum = evalClassCounts.entrySet().stream().mapToLong(e -> e.getValue()).sum();\n",
    "\n",
    "        // Determine the expected accuracies of the baselines\n",
    "        Map<String, double[]> f1ForClassGuessers = new HashMap<>();\n",
    "        for (Entry<String, Long> e : evalClassCounts.entrySet()) {\n",
    "            f1ForClassGuessers.put(e.getKey(), calcStats(e.getValue().intValue(),\n",
    "                    evaluationCorpus.size() - e.getValue().intValue(), (int) (expectedClassSum - e.getValue())));\n",
    "        }\n",
    "\n",
    "        // Train the classifier\n",
    "        long time1 = System.currentTimeMillis();\n",
    "        MultiLabelLearner learner = new MultiLabelLearner(classes);\n",
    "        for (ClassifiedDocument trainingExample : trainingCorpus) {\n",
    "            learner.learnExample(trainingExample.classes, trainingExample.text);\n",
    "        }\n",
    "        MultiLabelClassifier classifier = learner.createClassifier();\n",
    "        time1 = System.currentTimeMillis() - time1;\n",
    "        System.out.println(\"Training took       : \" + time1 + \"ms\");\n",
    "\n",
    "        // Classify the evaluation corpus\n",
    "        long time2 = System.currentTimeMillis();\n",
    "        Map<String, int[]> classCounts = new HashMap<>();\n",
    "        final int TP = 0, FP = 1, FN = 2, TN = 3;\n",
    "        for (String clazz : classes) {\n",
    "            classCounts.put(clazz, new int[4]);\n",
    "        }\n",
    "        int id = 0;\n",
    "        Set<String> result;\n",
    "        List<String[]> errorDetails = new ArrayList<>();\n",
    "        boolean added;\n",
    "        for (ClassifiedDocument evalExample : evaluationCorpus) {\n",
    "            added = false;\n",
    "            result = classifier.classify(evalExample.text);\n",
    "            String resultAsString = result.toString();\n",
    "            for (String clazz : classes) {\n",
    "                if (evalExample.classes.contains(clazz)) {\n",
    "                    if (result.contains(clazz)) {\n",
    "                        ++classCounts.get(clazz)[TP];\n",
    "                    } else {\n",
    "                        ++classCounts.get(clazz)[FN];\n",
    "                        if (!added) {\n",
    "                            errorDetails.add(new String[] { Integer.toString(id), evalExample.classes.toString(),\n",
    "                                    resultAsString });\n",
    "                        }\n",
    "                    }\n",
    "                } else {\n",
    "                    if (result.contains(clazz)) {\n",
    "                        ++classCounts.get(clazz)[FP];\n",
    "                        if (!added) {\n",
    "                            errorDetails.add(new String[] { Integer.toString(id), evalExample.classes.toString(),\n",
    "                                    resultAsString });\n",
    "                        }\n",
    "                    } else {\n",
    "                        ++classCounts.get(clazz)[TN];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            result.removeAll(evalExample.classes);\n",
    "            if ((result.size() > 0) && (!added)) {\n",
    "                errorDetails.add(\n",
    "                        new String[] { Integer.toString(id), evalExample.classes.toString(), result.toString() });\n",
    "            }\n",
    "            ++id;\n",
    "        }\n",
    "        time2 = System.currentTimeMillis() - time2;\n",
    "        System.out.println(\"Classification took : \" + time2 + \"ms\");\n",
    "        int counts[] = new int[4];\n",
    "        for (Entry<String, int[]> stats : classCounts.entrySet()) {\n",
    "            counts[0] += stats.getValue()[0];\n",
    "            counts[1] += stats.getValue()[1];\n",
    "            counts[2] += stats.getValue()[2];\n",
    "            counts[3] += stats.getValue()[3];\n",
    "        }\n",
    "        double solutionPerformance[] = calcStats(counts[TP], counts[FP], counts[FN]);\n",
    "\n",
    "        System.out.println(\"classifiers           precision    recall  f1-score\");\n",
    "        for (Entry<String, double[]> baseResult : f1ForClassGuessers.entrySet()) {\n",
    "            System.out.println(String.format(\"Always %-13s:   %-7.5f   %-7.5f   %-7.5f\", baseResult.getKey(),\n",
    "                    baseResult.getValue()[0], baseResult.getValue()[1], baseResult.getValue()[2]));\n",
    "        }\n",
    "        System.out.println(\n",
    "                String.format(\"Your solution       :   %-7.5f   %-7.5f   %-7.5f (%d tp, %d tn, %d fp, %d fn)\",\n",
    "                        solutionPerformance[0], solutionPerformance[1], solutionPerformance[2], counts[TP],\n",
    "                        counts[TN], counts[FP], counts[FN]));\n",
    "        f1 = solutionPerformance[2];\n",
    "        if (errorDetails.size() > 0) {\n",
    "            System.out.println(\"  Wrong classifications are:\");\n",
    "            for (int i = 0; i < Math.min(errorDetails.size(), 20); ++i) {\n",
    "                System.out.print(\"    id=\");\n",
    "                System.out.print(errorDetails.get(i)[0]);\n",
    "                System.out.print(\" expected=\");\n",
    "                System.out.print(errorDetails.get(i)[1]);\n",
    "                System.out.print(\" result=\");\n",
    "                System.out.println(errorDetails.get(i)[2]);\n",
    "            }\n",
    "            if (errorDetails.size() > 20) {\n",
    "                System.out.println(\"    ...\");\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Make sure that the students solution is better than all baselines\n",
    "        for (Entry<String, double[]> baseResult : f1ForClassGuessers.entrySet()) {\n",
    "            if (baseResult.getValue()[2] >= solutionPerformance[2]) {\n",
    "                StringBuilder builder = new StringBuilder();\n",
    "                builder.append(\"Your solution is not better than a classifier that always chooses the \\\"\");\n",
    "                builder.append(baseResult.getKey());\n",
    "                builder.append(\"\\\" class.\");\n",
    "                Assertions.fail(builder.toString());\n",
    "            }\n",
    "        }\n",
    "        if ((minF1Score > 0) && (minF1Score > solutionPerformance[2])) {\n",
    "            Assertions.fail(\"Your solution did not reach the expected F1-score of \" + minF1Score);\n",
    "        }\n",
    "        System.out.println(\"Test successfully completed.\");\n",
    "    } catch (AssertionFailedError e) {\n",
    "        throw e;\n",
    "    } catch (Throwable e) {\n",
    "        System.err.println(\"Your solution caused an unexpected error:\");\n",
    "        throw e;\n",
    "    }\n",
    "    return f1;\n",
    "}\n",
    "/**\n",
    " * Simple method for calculating micro precision, recall and F1-measure.\n",
    " */\n",
    "public static double[] calcStats(int tp, int fp, int fn) {\n",
    "    double precision = tp / (double) (tp + fp);\n",
    "    double recall = tp / (double) (tp + fn);\n",
    "    return new double[] { precision, recall, (2 * precision * recall) / (precision + recall) };\n",
    "}\n",
    "\n",
    "System.out.println(\"---------- Simple example corpus ----------\");\n",
    "List<ClassifiedDocument> exampleCorpusTrain = Arrays.asList(\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"chess\")),\n",
    "                \"white king, black rook, black queen, white pawn, black knight, white bishop.\"),\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"history\")),\n",
    "                \"knight person granted honorary title knighthood\"),\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"history\")),\n",
    "                \"knight order eligibility, knighthood, head of state, king, prelate, middle ages.\"),\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"chess\", \"game\")),\n",
    "                \"Defense knight king pawn opening game opponent.\"),\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"game\")),\n",
    "                \"Game. player opponent victory. draw.\"));\n",
    "List<ClassifiedDocument> exampleCorpusTest = Arrays.asList(\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"history\")), \"Knighthood Middle Ages.\"),\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"game\", \"chess\")),\n",
    "                \"player black knight opponent pawn queen checkmate game draw victory.\"),\n",
    "        // document with unknown words\n",
    "        new ClassifiedDocument(new HashSet<String>(Arrays.asList(\"game\")), \"player opponent opening\"));\n",
    "double f1Measure = checkClassifier(exampleCorpusTrain, exampleCorpusTest, 0);\n",
    "\n",
    "System.out.println();\n",
    "System.out.println(\"---------- Larger example corpus ----------\");\n",
    "List<ClassifiedDocument> classificationData = readClassData(\"/srv/distribution/multi-class-train.tsv\");\n",
    "f1Measure = checkClassifier(classificationData.subList(0, 600), classificationData.subList(600, classificationData.size()),\n",
    "        0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc4723f8815725c44713410c8d9ce586",
     "grade": true,
     "grade_id": "cell-autograde-1",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "// Ignore this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2280a65ff154fb7d12936e76235be051",
     "grade": true,
     "grade_id": "cell-autograde-2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "// Ignore this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ba1b93c7dc47996a5c81821c0c7c5cd",
     "grade": true,
     "grade_id": "cell-autograde-3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "// Ignore this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.4+11-post-Ubuntu-1ubuntu218.04.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
